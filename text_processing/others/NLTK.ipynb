{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The below materials are extracted from the NLTK book:\n",
    "<br>http://www.nltk.org/book/\n",
    "\n",
    "# 1 Language Processing and Python\n",
    "## 1.1.3 Searching Text\n",
    "- concordance: shows every occurrrence of a given word, together with some context.\n",
    "<br>in:word\n",
    "<br>out:contexts of the word occurring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 8 of 8 matches:\n",
      " , was called a Cape - Cod - man . A happy - go - lucky ; neither craven nor va\n",
      " rivers ; through sun and shade ; by happy hearts or broken ; through all the w\n",
      " most sea - terms , this one is very happy and significant . For the whale is i\n",
      "ng by way of getting a living . Oh ! happy that the world is such an excellent \n",
      "e says , Monsieur , that he ' s very happy to have been of any service to us .\"\n",
      "irst love ; we marry and think to be happy for aye , when pop comes Libra , or \n",
      " , a desperate burglar slid into his happy home , and robbed them all of everyt\n",
      "rous thing in his soul . That glad , happy air , that winsome sky , did at last\n"
     ]
    }
   ],
   "source": [
    "text1.concordance(\"happy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- similar: look for words appear in similar range of contexts of a given word.\n",
    "<br>\n",
    "in: word <br> out:simiar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "much that impossible well long soon and young civil before sorry far\n",
      "glad so she rich given sure unwilling great\n"
     ]
    }
   ],
   "source": [
    "text2.similar(\"happy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- common_contexts: contexts shared by two or more words\n",
    "<br>in: two or more words, enclosed by square brackets\n",
    "<br>out: common contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "very_to\n"
     ]
    }
   ],
   "source": [
    "text1.common_contexts([\"man\",\"happy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- dispersion_plot:display the location of a word in a text\n",
    "<br>in:one or more words, enclosed by square brackets \n",
    "<br>out:dispersion plot\n",
    "<br>note:can plot the frequency of word usage through time using ngrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "text1.dispersion_plot([\"happy\",\"quick\",\"freedom\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- generate: random text using a given text\n",
    "<br>in:textname before calling the function <br>out:random text\n",
    "<br>note:not available i NLTK 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.1.4 Counting Vocabulary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- set:obtain the vocabulary items\n",
    "    <br>in:text<br>out: vocabulary\n",
    "    <br>note: lexical diversity = len(set(text))/len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#sorted(set(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.count(\"what\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- FreqDist: frequency distribution of the words\n",
    "<br>in:text<br>out:list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 9397), ('to', 4063), ('.', 3975), ('the', 3861), ('of', 3565), ('and', 3350), ('her', 2436), ('a', 2043), ('I', 2004), ('in', 1904)]\n"
     ]
    }
   ],
   "source": [
    "fdist1=FreqDist(text2)\n",
    "print(fdist1.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.3.2 Fine-grained Selection of Words\n",
    "- extract words by length and frequency counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#long_words = [w for w in V if len(w) > 15]\n",
    "#sorted(w for w in set(text5) if len(w) > 7 and fdist5[w] > 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.3.3 Collocations and Bigrams\n",
    "- Collocations and Bigrams\n",
    "collocations are sequences of words that occur together usually often. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonel Brandon; Sir John; Lady Middleton; Miss Dashwood; every thing;\n",
      "thousand pounds; dare say; Miss Steeles; said Elinor; Miss Steele;\n",
      "every body; John Dashwood; great deal; Harley Street; Berkeley Street;\n",
      "Miss Dashwoods; young man; Combe Magna; every day; next morning\n"
     ]
    }
   ],
   "source": [
    "text2.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.3.4 NLTK's frequency Distributions\n",
    "fdist = FreqDist(samples)   #create a frequency distribution containing the given samples\n",
    "<br>fdist[sample] += 1          #increment the count for this sample\n",
    "<br>fdist['monstrous']           #count of the number of times a given sample occurred\n",
    "<br>fdist.freq('monstrous')           #frequency of a given sample\n",
    "<br>fdist.N()                      #total number of samples\n",
    "<br>fdist.most_common(n)           #the n most common samples and their frequencies\n",
    "<br>for sample in fdist:           #iterate over the samples\n",
    "<br>fdist.max()                     # #sample with the greatest count\n",
    "<br>fdist.tabulate()           #tabulate the frequency distribution\n",
    "<br>fdist.plot()                      #graphical plot of the frequency distribution\n",
    "<br>fdist.plot(cumulative=True)           #cumulative plot of the frequency distribution\n",
    "<br>fdist1 |= fdist2                      #update fdist1 with counts from fdist2\n",
    "<br>fdist1 < fdist2            #test if samples in fdist1 occur less frequently than in fdist2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.3.5 Word Comparison Operators\n",
    "s.startswith(t)\ttest if s starts with t\n",
    "<br>s.endswith(t)\ttest if s ends with t\n",
    "<br>t in s\t    test if t is a substring of s\n",
    "<br>s.islower()\ttest if s contains cased characters and all are lowercase\n",
    "<br>s.isupper()\ttest if s contains cased characters and all are uppercase\n",
    "<br>s.isalpha()\ttest if s is non-empty and all characters in s are alphabetic\n",
    "<br>s.isalnum()\ttest if s is non-empty and all characters in s are alphanumeric\n",
    "<br>s.isdigit()\ttest if s is non-empty and all characters in s are digits\n",
    "<br>s.istitle()\ttest if s contains cased characters and is titlecased (i.e. all words in s have initial capitals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2. Accessing Text Corpora and Lexical Resources\n",
    "## 2.1.1 Gutenberg Corpus \n",
    "http://www.gutenberg.org/  25000 free electronic books\n",
    "## 2.1.2 Web and Chat text\n",
    "content from a Firefox discussion forum, conversations overheard in New York, the movie script of Pirates of the Carribean, personal advertisements, and wine reviews\n",
    "- refers to the book for more corpus\n",
    "\n",
    "## 2.4.1 Unusual words\n",
    "computes the vocabulary of a text, then removes all items that occur in an existing wordlist, leaving just the uncommon or mis-spelt words.\n",
    "<br>in: corpus\n",
    "<br>out: list of unusual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abated', 'abating', 'abednego', 'abhorred', 'abided', 'abjectus', 'ablutions', 'abominated', 'aboriginalness', 'abortions']\n"
     ]
    }
   ],
   "source": [
    "def unusual_words(text):\n",
    "    text_vocab = set(w.lower() for w in text if w.isalpha())\n",
    "    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "    unusual = text_vocab - english_vocab\n",
    "    return sorted(unusual)\n",
    "\n",
    "print(unusual_words(text1)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2.4.1 Stopwords \n",
    "high-frequency words like the, to and also that we sometimes want to filter out of a document before further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2.4.4 Shoebox and Toolbox Lexicons\n",
    "consists of a collection of entries, where each entry is made up of one or more fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('kaakaaro', [('ps', 'N'), ('pt', 'NT'), ('ge', 'mixture'), ('tkp', '???'), ('eng', 'mixtures'), ('eng', 'charm used to keep married men and women youthful and attractive'), ('cmt', 'Check vowel length. Is it kaakaaro or kaakaro? Does lexeme have suffix, -aro or -ro?'), ('dt', '20/Nov/2006'), ('ex', 'Kaakaroto ira purapaiveira aue iava opita, voeao-pa airepa oraouirara, ra va aiopaive.'), ('xp', 'Kokonas ol i save wokim long ol kain samting bilong ol nupela marit, bai ol i ken kaikai.'), ('xe', 'Mixtures are made from coconut for newlyweds, who eat them.')])\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import toolbox\n",
    "print(toolbox.entries('rotokas.dic')[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2.5 Wordnets\n",
    "find Synonyms: \n",
    "<br>in: word <br> out: word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('happy.a.01'),\n",
       " Synset('felicitous.s.02'),\n",
       " Synset('glad.s.02'),\n",
       " Synset('happy.s.04')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('happy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enjoying or showing or marked by joy or pleasure\n",
      "['a happy smile', 'spent many happy days on the beach', 'a happy marriage']\n"
     ]
    }
   ],
   "source": [
    "#definitions\n",
    "print(wn.synset('happy.a.01').definition())\n",
    "#examples\n",
    "print(wn.synset('happy.a.01').examples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- Other lexical relations\n",
    "<br>hyponyms: more specific\n",
    "<br>hypernyms:navigate up the hierarchy\n",
    "<br>meronyms: components\n",
    "<br>holonyms: things the words are contained in \n",
    "<br>word1.path_similarity(word2):caluculate semantic similarity based on paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3 Processing Raw Text\n",
    "## 3.1 Tokenize word\n",
    "word_tokenize\n",
    "<br>in:texts<br>out: list of words\n",
    "<br><br>Text:create a text from tokens\n",
    "<br>in:tokens<br>out:text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3.6 Normalizing Text\n",
    "Stemmers:strip off any affixes\n",
    "- PorterStemmer\n",
    "- LancasterStemmer\n",
    "\n",
    "<br>Lemmatization:removes affixes if the resulting word is in its dictionary\n",
    "- WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']', 'ETYMOLOGY', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "print([wnl.lemmatize(t) for t in text1][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3.7 NLTK's regular expression tokenizer\n",
    "nltk.regexp_tokenize: tokenizer using regular expression \n",
    "<br>in:text, pattern<br>out:tokens\n",
    "\n",
    "## 3.8 Segmentation\n",
    "- sentence segmentation\n",
    "<br>nltk.sent_tokenize:<br>in:text<br>out:sentences\n",
    "\n",
    "# 5 Categorizing and Tagging words\n",
    "## 5.1 Using a Tagger\n",
    "- Pos-tagger: processes a sequence of words, and attaches a part of speech tag to each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('From', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('day', 'NN'),\n",
       " ('till', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('end', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('all', 'DT'),\n",
       " ('days', 'NNS')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens=nltk.word_tokenize(\"From this day till the end of all days\")\n",
    "nltk.pos_tag(tokens)\n",
    "#some nltk packages have tagged words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5.5 N-Gram Tagging\n",
    "- Unigram Tagging\n",
    "<br>nltk.UnigramTagger: most frequent tag for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Various', 'JJ'), ('of', 'IN'), ('the', 'AT'), ('apartments', 'NNS'), ('are', 'BER'), ('of', 'IN'), ('the', 'AT'), ('terrace', 'NN'), ('type', 'NN'), (',', ','), ('being', 'BEG'), ('on', 'IN'), ('the', 'AT'), ('ground', 'NN'), ('floor', 'NN'), ('so', 'QL'), ('that', 'CS'), ('entrance', 'NN'), ('is', 'BEZ'), ('direct', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "brown_sents = brown.sents(categories='news')\n",
    "unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n",
    "print(unigram_tagger.tag(brown_sents[2007]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- n-gram tagger: a generalization of a unigram tagger whose context is the current word together with the part-of-speech tags of the n-1 preceding tokens\n",
    "<br>n=2: nltk.BigramTagger\n",
    "<br>note:accuracy score is low when number of words are not present in the training data(the word will be assigned \"None\")\n",
    "- combine tagger: combine default, unigram and n-gram taggers to address the trade-off between accuracy and coverage. Use specified backoff taggers\n",
    "- Brill_tagger: guess the tag of each word, then go back and fix the mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#t0 = nltk.DefaultTagger('NN')\n",
    "#t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "#t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "#t2.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 6. Learning to Classify Text\n",
    "## 6.1 Supervised Classification\n",
    "- NaiveBayesClassifier\n",
    "<br>train,classify,prob_classify,show_most_informative_features\n",
    "- DecisionTreeClassifier\n",
    "\n",
    "## 6.3 Evaluation\n",
    "- classify.accuracy \n",
    "<br>in:prediction,test set <br>out:accuracy\n",
    "- ConfusionMatrix\n",
    "\n",
    "# 7. Extracting information from Text\n",
    "## 7.2.1 Noun Phrase Chunking\n",
    "RegexpParser(grammar):\n",
    "use a grammar to parse the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "#this example shows noun Chunking\n",
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), \n",
    "... (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\" \n",
    "cp = nltk.RegexpParser(grammar) \n",
    "result = cp.parse(sentence)\n",
    "print(result)\n",
    "result.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 7.3.2 Simple Evaluation and Baselines: IOB tagging\n",
    "- use RegexpParser to evaluate the performance of chunked sentences\n",
    "<br>I: in the chunk<br>O: not in a chunk <br> B:the tag is the beginning of a chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  87.7%%\n",
      "    Precision:     70.6%%\n",
      "    Recall:        67.8%%\n",
      "    F-Measure:     69.2%%\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"NP: {<[CDJNP].*>+}\"\n",
    "from nltk.corpus import conll2000\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "print(cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 7.4.2 Trees\n",
    "create a tree by giving a node label and a list of children:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP Alice) (NP the rabbit))\n",
      "NP\n",
      "['Alice', 'the', 'rabbit']\n"
     ]
    }
   ],
   "source": [
    "tree1 = nltk.Tree('NP', ['Alice'])\n",
    "tree2 = nltk.Tree('NP', ['the', 'rabbit'])\n",
    "#incorporate into larger trees\n",
    "tree4 = nltk.Tree('S', [tree1, tree2])\n",
    "print(tree4)\n",
    "print(tree4[1].label())\n",
    "print(tree4.leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 7.5 Named Entity Recognition (NER)\n",
    "definite noun phrases that refer to specific types of individuals, such as organizations, persons, dates, time, money, percent, facility geo-political entities like city, state, country, and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Mr./NNP)\n",
      "  (PERSON Vinken/NNP)\n",
      "  is/VBZ\n",
      "  chairman/NN\n",
      "  of/IN\n",
      "  (ORGANIZATION Elsevier/NNP)\n",
      "  N.V./NNP\n",
      "  ,/,\n",
      "  the/DT\n",
      "  (GPE Dutch/NNP)\n",
      "  publishing/VBG\n",
      "  group/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import chunk\n",
    "sent = nltk.corpus.treebank.tagged_sents()[1]\n",
    "print(nltk.ne_chunk(sent))#if binary = True, return NE and non NE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 8. Analyzing Sentence Structure\n",
    "## 8.4.1 Recursive Descent Parsing (Top down)\n",
    "With the initial goal (find an S), the S root node is created. As the above process recursively expands its goals using the productions of the grammar, the parse tree is extended downwards (hence the name recursive descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP Mary) (VP (V saw) (NP (Det a) (N dog))))\n"
     ]
    }
   ],
   "source": [
    "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  VP -> V NP | V NP PP\n",
    "  PP -> P NP\n",
    "  V -> \"saw\" | \"ate\" | \"walked\"\n",
    "  NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n",
    "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
    "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
    "  P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
    "  \"\"\")\n",
    "rd_parser = nltk.RecursiveDescentParser(grammar1)\n",
    "sent = 'Mary saw a dog'.split()\n",
    "for tree in rd_parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- short coming of Recursive Descent Parsing: <br>\n",
    "First, left-recursive productions like NP -> NP PP send it into an infinite loop. \n",
    "<br>Second, the parser wastes a lot of time considering words and structures that do not correspond to the input sentence. \n",
    "<br>Third, the backtracking process may discard parsed constituents that will need to be rebuilt again later.\n",
    "\n",
    "## 8.4.2 Shift-Reduce Parsing (Bottom up)\n",
    "tries to find sequences of words and phrases that correspond to the right hand side of a grammar production, and replace them with the left-hand side, until the whole sentence is reduced to an S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP Mary) (VP (V saw) (NP (Det a) (N dog))))\n"
     ]
    }
   ],
   "source": [
    "sr_parser = nltk.ShiftReduceParser(grammar1)\n",
    "sent = 'Mary saw a dog'.split()\n",
    "for tree in sr_parser.parse(sent):\n",
    "     print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- short coming of Shift-Reduce Parsing: <br>\n",
    "A shift-reduce parser can reach a dead end and fail to find any parse\n",
    "- advantage over shift-reduce parsers: <br>\n",
    "only build structure that corresponds to the words in the input. Furthermore, they only build each sub-structure once\n",
    "\n",
    "## 8.4.3 The Left-Corner Parser （hybrid）\n",
    "Top-down parser with bottom-up filtering \n",
    "\n",
    "## 8.5 Dependencies and Dependency Grammar\n",
    "dependency grammar focusses on how words relate to other words. Dependency is a binary asymmetric relation that holds between a head and its dependents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Other Important Modules from NLTK package\n",
    "https://www.nltk.org/py-modindex.html#\n",
    "## Chat: different chatbots\n",
    "examples: eliza, iesha,rude,suntsu, util,zen\n",
    "## Combinatory Categorial Grammar\n",
    "- ccg.api \n",
    "- ccg.chart:\n",
    "<br>standard English rules: chart.DefaultRuleSet\n",
    "<br>construct parser by calling: parser=chart.CCGChartParser(<lexicon>,<ruleset>)\n",
    "<br>\n",
    "nltk.ccg.chart.compute_semantics(children, edge)\n",
    "<br>\n",
    "nltk.ccg.chart.printCCGDerivation(tree)\n",
    "<br>nltk.ccg.chart.printCCGTree(lwidth, tree)\n",
    "- ccg.combinator\n",
    "- ccg.lexicon: create lexicon for CCG "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Chunk\n",
    "- nltk.chunk.regexp.ChinkRule(tag_pattern, descr)\n",
    "<br>find any substring that matches the tag pattern and that is contained in a chunk, and remove it from that chunk, thus creating two new chunks\n",
    "- nltk.chunk.regexp.ChunkRule(tag_pattern, descr)\n",
    "<br>find any substring that matches this tag pattern and that is not already part of a chunk, and create a new chunk containing that substring\n",
    "- nltk.chunk.regexp.ChunkRuleWithContext(left_context_tag_pattern, chunk_tag_pattern, right_context_tag_pattern,descr)\n",
    "<br>\n",
    "\n",
    "## Classify\n",
    "in: features, labels <br>out: classfication model<br>\n",
    "- nltk.classify.positivenaivebayes\n",
    "<br>A variant of the Naive Bayes Classifier that performs binary classification with partially-labeled training sets: when the training set has only one incomplete labelled class. \n",
    "- nltk.classify.scikitlearn\n",
    "<br>implement a wrapper around scikit-learn classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "classif = SklearnClassifier(LinearSVC())\n",
    "#A scikit-learn classifier may include preprocessing steps when it’s wrapped in a Pipeline object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "- nltk.classify.textcat\n",
    "<br>A module for language identification using the TextCat algorithm\n",
    "\n",
    "## cluster\n",
    "- contains a number of basic clustering algorithms: k-means, E-M clusterer and group average agglomerative clusterer.\n",
    "\n",
    "## corpus\n",
    "- used to read corpus files in a variety of formats. The functions are named based on the type of information they return:\n",
    "<br>words(): list of str\n",
    "<br>sents(): list of (list of str)\n",
    "<br>paras(): list of (list of (list of str))\n",
    "<br>tagged_words(): list of (str,str) tuple\n",
    "<br>tagged_sents(): list of (list of (str,str))\n",
    "<br>tagged_paras(): list of (list of (list of (str,str)))\n",
    "<br>chunked_sents(): list of (Tree w/ (str,str) leaves)\n",
    "<br>parsed_sents(): list of (Tree with str leaves)\n",
    "<br>parsed_paras(): list of (list of (Tree with str leaves))\n",
    "<br>xml(): A single xml ElementTree\n",
    "<br>raw(): unprocessed corpus contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The, Fulton, County, Grand, Jury, said, Friday, an, investigation, of\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "print(\", \".join(brown.words()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Download\n",
    "download corpora, models, and other data packages that can be used with NLTK\n",
    "- download()\n",
    "<br>display an interactive interface\n",
    "- download(\"package name\")\n",
    "- Downloader.default_download_dir()\n",
    "<br>see download directory\n",
    "\n",
    "## Draw\n",
    "displaying cfg, lexical dispersion, table, tree\n",
    "\n",
    "## Feature Structures\n",
    "representing feature structures, and for performing basic operations on those feature structures\n",
    "- FeatDict: \n",
    "<br> feature dictionaries, Feature identifiers may be strings or instances of the Feature class\n",
    "- FeatList:\n",
    "<br>Feature lists, feature identifiers are integers\n",
    "- Feature structure variables are encoded using the nltk.sem.Variable class\n",
    "- it is possible to track the bindings of variables if you choose to, by supplying your own initial bindings dictionary to the unify() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 'a', 'x': 1, 'y': {'b': 'b'}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.featstruct import unify\n",
    "unify(dict(x=1, y=dict()), dict(a='a', y=dict(b='b')))  \n",
    "{'y': {'b': 'b'}, 'x': 1, 'a': 'a'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar\n",
    "Basic data classes for representing context free grammars. A “grammar” specifies which trees can represent the structure of a given text. Each of these trees is called a “parse tree” for the text (or simply a “parse”). In this context, the leaves of a parse tree are word tokens; and the node values are phrasal categories, such as NP and VP.\n",
    "\n",
    "## Metrics\n",
    "association, confusionmatrix, distance, socres, segmentation, spearman correlation coefficient\n",
    "\n",
    "## Parsers\n",
    "Classes and interfaces for producing tree structures that represent the internal organization of a text\n",
    "\n",
    "## probability \n",
    "Classes for representing and processing probabilistic information.\n",
    "\n",
    "- FreqDist\n",
    "<br>class is used to encode “frequency distributions”\n",
    "- ProbDistI\n",
    "<br>defines a standard interface for “probability distributions”\n",
    "- ConditionalFreqDist and ConditionalProbDistI\n",
    "<br>used to encode conditional distributions.\n",
    "\n",
    "## Semantic interpretation\n",
    "- logic \n",
    "<br>provides support for analyzing expressions of First Order Logic (FOL).\n",
    "- evaluate \n",
    "<br>allows users to recursively determine truth in a model for formulas of FOL.\n",
    "\n",
    "## Stem\n",
    "remove morphological affixes from words, leaving only the word stem. \n",
    "- nltk.tag.pos_tag(tokens, tagset=None, lang='eng')\n",
    "<br>tagset: the tagset to be used\n",
    "<br>lang: language\n",
    "<br>out:tagged tokens\n",
    "<br>return type:list(tuple(str, str))\n",
    "- nltk.tag.pos_tag_sents(sentences, tagset=None, lang='eng')\n",
    "<br>return type:list(list(tuple(str, str)))\n",
    "\n",
    "## Tokenize\n",
    "Tokenizers divide strings into lists of substrings\n",
    "\n",
    "## tree\n",
    "Class for representing hierarchical language structures, such as syntax trees and morphological trees."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
